{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjN50ea5qk_B",
    "outputId": "7d35d5ec-a707-4a4e-bbf6-7cac98583171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXZxZS8Ca4G8"
   },
   "source": [
    "# ***IMPORTS BLOCK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWVBEqAMa4QY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from utils.visualization import visualize_hand\n",
    "from utils.general import set_seed, get_device, print_data_info, print_model_info, save_training_arguments, logger, save_checkpoint, load_checkpoint\n",
    "from utils.data import generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsq6z8X16yF-"
   },
   "source": [
    "# ***MODEL UTILS BLOCK:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laWo0up667lh"
   },
   "outputs": [],
   "source": [
    "def load_encoder(latent_space_dim):\n",
    "    encoder = rgb_encoder(latent_space_dim) \n",
    "    return(encoder)\n",
    "\n",
    "class rgb_encoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim):\n",
    "        super(rgb_encoder, self).__init__()\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        #(1) Load ResNet-18, pretrained in Imagenet from torchvision.models and remove the last fully connected layer\n",
    "        #(2) Build a new last fully connected layer, that takes 512 features + 2 (handedness indicator)and returns 2xlatent space dimensionality (mean and log-variance of the predicted distribution) \n",
    "        model = models.resnet18(pretrained=True)     \n",
    "        self.encoder =nn.Sequential(*(list(model.children())[:-1]))\n",
    "        self.fc = nn.Linear(in_features=514,out_features=2*self.latent_space_dim) \n",
    "    def forward(self, image, hand_side):\n",
    "        x = self.encoder(image).squeeze()\n",
    "        z = torch.cat((x.float(),hand_side.float()),dim=1)\n",
    "        x = self.fc(z)\n",
    "        return x[:, self.latent_space_dim:], x[:, :self.latent_space_dim]\n",
    "\n",
    "def load_decoder(latent_space_dim, num_of_joints):\n",
    "    decoder = pose_decoder(latent_space_dim, num_of_joints)\n",
    "    return decoder\n",
    "\n",
    "class pose_decoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, joint_num):\n",
    "        super(pose_decoder, self).__init__()\n",
    "        self.joint_num = torch.IntTensor(joint_num) # [21,3]\n",
    "        self.in_size = self.joint_num.prod() # 63\n",
    "        #(1): Build the pose decoder\n",
    "        self.lin_lays = nn.Sequential(\n",
    "            #first layer\n",
    "            nn.Linear(in_features=64,out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            #second layer\n",
    "            nn.Linear(in_features=512,out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            #third layer\n",
    "\n",
    "            nn.Linear(in_features=512,out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            #fourth layer           \n",
    "            nn.Linear(in_features=512,out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            #fifth layer\n",
    "            nn.Linear(in_features=512,out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            #last layer\n",
    "            nn.Linear(in_features=512,out_features=int(self.in_size))\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        out_lays = self.lin_lays(sample)\n",
    "        return out_lays.view(-1, self.joint_num[0], self.joint_num[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiYZ40to7XpQ"
   },
   "source": [
    "\n",
    "# ***CROSS MODEL BLOCK***\n",
    "This block creates the VAE model using the above encoder/decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1YNQRVQ7ax8"
   },
   "outputs": [],
   "source": [
    "class VAE_model(nn.Module):\n",
    "    def __init__(self, rgb_encoder, pose_decoder, num_of_joints, latent_space_dim):\n",
    "        super(VAE_model, self).__init__()\n",
    "        self.encoder = rgb_encoder\n",
    "        self.decoder = pose_decoder\n",
    "        self.latent_space_dim =latent_space_dim\n",
    "    def forward(self, x, hand_side):\n",
    "        #(1) Obtain mean and logvar by encoding the input image and concatenating the handedness into encoded features\n",
    "        self.mean,self.var = self.encoder.forward(x, hand_side)\n",
    "        #(2) Reparameterize to sample from the latent space-distribution\n",
    "        self.repair = self.reparameterize(self.mean,self.var)\n",
    "        #(3) Decode the sample \n",
    "        self.decoding = self.decoder.forward(self.repair)\n",
    "        return self.decoding\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I2R4MvSix_M"
   },
   "source": [
    "# ***LOSSES BLOCK***\n",
    "This block calculates the MSE and KL losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGmz_AexiyH-"
   },
   "outputs": [],
   "source": [
    "def get_mse_loss(preds, ground_truth):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    loss = criterion(preds,ground_truth)\n",
    "    return loss \n",
    "\n",
    "def get_kl_loss(mean, logvar):\n",
    "\n",
    "    kl_loss = - 0.5 * torch.sum(1 + logvar - mean**2 -torch.exp(logvar))\n",
    "    return kl_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4IG0sxEiYfP"
   },
   "source": [
    "# ***METRIC BLOCK***\n",
    "This block implements mean End-Point-Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUOzbbciiY1Q"
   },
   "outputs": [],
   "source": [
    "def calc_mean_epe(pred_joints, gt_joints, visible_joints):\n",
    "lculate mean EPE on all joints \n",
    "\n",
    "    We calculate mean EPE on visible joints \n",
    "\n",
    "    It return mean EPE on visible keypoints\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue8YCDxOo2Ss"
   },
   "source": [
    "# ***TRAINING BLOCK:***\n",
    "This block contains the training function that you will use for the training process of the VAE model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAALrkrukK-x"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, optimizer, epoch, beta_kl, total_epochs, log_interval):    \n",
    "\n",
    "    model.train()\n",
    "    train_mse_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    mean_epe = 0\n",
    "    for batch_idx, (img, keypoint_xyz21, keypoint_vis21, keypoint_scale, hand_side) in enumerate(train_loader):\n",
    "      # send the img data set to gpu device\n",
    "      img=img.float()\n",
    "      img = img.to(device)\n",
    "      keypoint_xyz21 = keypoint_xyz21.float()\n",
    "      keypoint_xyz21 = keypoint_xyz21.to(device)\n",
    "      hand_side=hand_side.float()\n",
    "      hand_side = hand_side.to(device)\n",
    "      #(1) Forward propagation of input to the model\n",
    "      logits = model.forward(img,hand_side)\n",
    "      keypoint_scale = torch.unsqueeze(keypoint_scale,1)\n",
    "      keypoint_scale = torch.unsqueeze(keypoint_scale,2)\n",
    "      keypoint_scale=keypoint_scale.to(device)\n",
    "      #(2) Make predictions scale-invariant (multiply the keypoint predictions with the calculated bone scale (“keypoint_scale”), in order to make them scale invariant)\n",
    "      #  x1 = torch.tensor(np.ones(shape=(logits.shape[0],logits.shape[1],logits.shape[2])))\n",
    "      #  for i in range(logits.shape[0]):\n",
    "      #    x1[i] = x1[i] * keypoint_scale[i]\n",
    "      logits *=  keypoint_scale\n",
    "       #(3) Loss Calculation\n",
    "      mse_loss = get_mse_loss(logits,keypoint_xyz21)\n",
    "      train_mse_loss += mse_loss.item()\n",
    "    \n",
    "      mean   = model.mean\n",
    "      logvar = model.var\n",
    "      kl_loss  = get_kl_loss(mean,logvar)\n",
    "      train_kl_loss += kl_loss.item()\n",
    "      #mse_loss  = mse_loss.to(torch.float32)\n",
    "     # kl_loss  =kl_loss.to(torch.float32)\n",
    "      loss = mse_loss + beta_kl * kl_loss\n",
    "       #(4) Backpropagation and optimization\n",
    "       # Back-propagation of loss and gradient calculation\n",
    "      loss.backward()\n",
    "        # optimization\n",
    "      optimizer.step()\n",
    "        # zero the gradients buffer\n",
    "      optimizer.zero_grad()\n",
    "       #(5) Mean EPE calculation of visible keypoints\n",
    "      mean_epe = calc_mean_epe(logits, keypoint_xyz21, keypoint_vis21)\n",
    "\n",
    "\n",
    "        #print stats per log_interval*batch_size samples: mean epe*1000 to express in mm, losses divided by number of samples assuming you calculate them with reduction \"sum\" (not mean per batch)\n",
    "      if (batch_idx % log_interval == 1):\n",
    "         print('Train Epoch: [{}/{}] \\t Image [{}/{}] \\t Mean_EPE: {:.6f} \\t Loss_MSE: {:.6f} \\tLoss_KL: {:.6f}'.format(\n",
    "                epoch,total_epochs, batch_idx * len(img), len(train_loader.dataset), \\\n",
    "                1000*mean_epe/(batch_idx+1),\\\n",
    "                train_mse_loss / ((batch_idx + 1)*len(img)),\\\n",
    "                train_kl_loss / ((batch_idx + 1)*len(img))))\n",
    "\n",
    "    return [str(epoch), str(1000*mean_epe/(batch_idx+1)),\n",
    "              str(float(train_mse_loss / len(train_loader.dataset))), str(float(train_kl_loss / len(train_loader.dataset)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnBvrPu3yOsN"
   },
   "source": [
    "# ***EVALUATION BLOCK:***\n",
    "This block contains the validation function that you will use for the training process of the VAE model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoRdWHFqkZxv"
   },
   "outputs": [],
   "source": [
    "def validate(model, test_loader, device, optimizer, epoch, ckpt_dir):\n",
    "\n",
    "    model.eval()\n",
    "    test_mse_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    mean_epe = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, (img, keypoint_xyz21, keypoint_vis21, keypoint_scale, hand_side) in enumerate(test_loader):\n",
    "          \n",
    "          img=img.float()\n",
    "          img = img.to(device)\n",
    "          keypoint_xyz21 = keypoint_xyz21.float()\n",
    "          keypoint_xyz21 = keypoint_xyz21.to(device)\n",
    "          hand_side=hand_side.float()\n",
    "          hand_side = hand_side.to(device)\n",
    "          #(1) Forward propagation of input to the model\n",
    "          logits = model.forward(img,hand_side)\n",
    "          keypoint_scale = torch.unsqueeze(keypoint_scale,1)\n",
    "          keypoint_scale = torch.unsqueeze(keypoint_scale,2)\n",
    "          keypoint_scale=keypoint_scale.to(device)\n",
    "          #(2) Make predictions scale-invariant (multiply the keypoint predictions with the calculated bone scale (“keypoint_scale”), in order to make them scale invariant)\n",
    "          logits *=  keypoint_scale\n",
    "          xyz_prediction = logits\n",
    "          #    (3) Loss Calculation\n",
    "          mse_loss = get_mse_loss(logits,keypoint_xyz21)\n",
    "          test_mse_loss += mse_loss.item()\n",
    "          mean   = model.mean\n",
    "          logvar = model.var\n",
    "          kl_loss  = get_kl_loss(mean,logvar)\n",
    "          test_kl_loss += kl_loss.item()\n",
    "          #(4) Mean EPE calculation of visible keypoints\n",
    "          mean_epe = calc_mean_epe(logits, keypoint_xyz21, keypoint_vis21)\n",
    "\n",
    "          #(5) Visualize the predicted and ground-truth keypoints alongside each image \n",
    "        \n",
    "          visualize_hand(img.clone().detach(), xyz_prediction.clone().detach().cpu().numpy(), keypoint_xyz21.clone().detach().cpu().numpy(),ckpt_dir,batch_idx)\n",
    "\n",
    "    print('Evaluation : Mean_EPE {:.5f}\\t Loss_MSE: {:.6f} \\tLoss_KL: {:.6f}'.format(\n",
    "         1000*mean_epe/(batch_idx+1),\\\n",
    "         test_mse_loss / len(test_loader.dataset),\\\n",
    "         test_kl_loss / len(test_loader.dataset)))\n",
    "\n",
    "    return 1000*mean_epe/(batch_idx+1), [str(epoch),\n",
    "                   str(1000*mean_epe/(batch_idx+1)), str(float(test_mse_loss /  len(test_loader.dataset))), str(float(test_kl_loss /  len(test_loader.dataset)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENQYKpUk0oCA"
   },
   "source": [
    "# ***MAIN BLOCK:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn-E7Ay9eEaD"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    timestamp = str(time.asctime(time.localtime(time.time())))\n",
    "    parser = argparse.ArgumentParser(description='3D Hand Pose Estimation')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "\n",
    "    parser.add_argument('--beta-kl', type=float, default=0.00001, metavar='b',\n",
    "                        help='weight of KL loss(default: 0.00001)')\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                        help='number of epochs to train (default: 1000)')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0001)')\n",
    "    \n",
    "    parser.add_argument('--z-dim', type=int, default=64, metavar='N',\n",
    "                        help='latent space dimensionality (default: 64)')\n",
    "\n",
    "    parser.add_argument('--pretrained', type=bool, default=True, metavar='p',\n",
    "                        help='load pretrained model')\n",
    "    \n",
    "    parser.add_argument('--cuda', action='store_true', default=True,\n",
    "                        help='enables CUDA training')\n",
    "\n",
    "    parser.add_argument('--mode', type=str, default='rgb23d', metavar='m',\n",
    "                        help='VAE mode')\n",
    "    \n",
    "    parser.add_argument('--seed', type=int, default=1234, metavar='S',\n",
    "                        help='random seed (default: 1234)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='l',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--gpu', type=str, default='0')\n",
    "\n",
    "    args  = parser.parse_args(args=[])\n",
    "\n",
    "    timestamp = str(time.asctime(time.localtime(time.time())))\n",
    "    set_seed(args.seed)\n",
    "    device = get_device(args.cuda, args.gpu)\n",
    "    ###Print data settings###\n",
    "    print_data_info()\n",
    "    ###Print model settings###\n",
    "  #  print_model_info(args.beta_kl, args.z_dim, args.lr, args.batch_size, args.log_interval)\n",
    "    ###Generate training and test data###\n",
    "    training_generator, test_generator = generate_data(args.batch_size)\n",
    "\n",
    "    num_of_joints=[21,3]\n",
    "    ###Create VAE model###/\n",
    "    rgb_encoder = load_encoder(args.z_dim)\n",
    "    pose_decoder = load_decoder(args.z_dim, num_of_joints)\n",
    "    model = VAE_model(rgb_encoder, pose_decoder, num_of_joints, args.z_dim).to(device)\n",
    "    ###Create optimizer###\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    ###Create checkpoint folder###\n",
    "    ckpt_fol_name = '/content/drive/My Drive/hand_checkpoints/test_day' + timestamp\n",
    "\n",
    "    \"\"\"\n",
    "    PRETRAINED SECTION\n",
    "    \"\"\"\n",
    "    if args.pretrained:\n",
    "      ###Choose pretrained path to load###\n",
    "      path = \"/content/drive/MyDrive/hand_checkpoints/test_daySun Jan  3 23_22_41 2021/best_mean_epe.pth\"\n",
    "      print(\"Resuming pretrained model\")\n",
    "      start_epoch = load_checkpoint(path, model)\n",
    "      start_epoch = start_epoch + 1\n",
    "    else:\n",
    "      start_epoch = 1\n",
    "    print(timestamp)\n",
    "    print(\"Checkpoint Directory= \", ckpt_fol_name)\n",
    "\n",
    "    ##Training-test loop###\n",
    "\n",
    "    best_mean_epe = 1000000\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        # \"TRAINING MODE\"\n",
    "        # print(\"!!!!!!!!   TRAINING   !!!!!!!!\")\n",
    "        # train_stats = train(model, training_generator, device, optimizer, epoch, args.beta_kl, args.epochs, args.log_interval)\n",
    "        \n",
    "        \"EVALUATION MODE\"\n",
    "        print(\"!!!!!!!!   VALIDATION   !!!!!!!!\")\n",
    "        mean_epe, test_stats = validate(model, test_generator, device, optimizer, epoch, ckpt_fol_name)\n",
    "      \n",
    "        \"CHECKPOINT\"\n",
    "        is_best = mean_epe < best_mean_epe\n",
    "        if (is_best):\n",
    "            print(\"Best mean_EPE {}\".format(mean_epe))\n",
    "            best_mean_epe = mean_epe\n",
    "            save_checkpoint(model, epoch, mean_epe, ckpt_fol_name, 'best_mean_epe')\n",
    "        else:\n",
    "            save_checkpoint(model, epoch, mean_epe, ckpt_fol_name, 'last')\n",
    "\n",
    "        ###save training measurements###\n",
    "        logger(ckpt_fol_name + '/training.txt', train_stats)\n",
    "        ###save test measurements###\n",
    "        logger(ckpt_fol_name + '/validation.txt', test_stats)\n",
    "        ###save arguments###\n",
    "        save_training_arguments(args,ckpt_fol_name)\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hand_pose_students_exercise_1__project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
